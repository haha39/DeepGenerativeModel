Run LLM inferences.

# Use ollama to test LLM models
1. Choose 2 models on ollama, for example, gemma 3 4b-it., geCompamma3 1b-it
2. Choose 3 different quantization weights(e.g. fp 16, q4_K_M, q8_0)
3. Try on GPU and on CPU(Can use colab free GPU.)

So you will have 2x3x2=12 combinations

Compare the speed (tokens per second.) of these settings.
* prompt eval rate
* eval rate

Design your own prompt.

Submit your result in a document format with screenshots for each of the 12 experiments.