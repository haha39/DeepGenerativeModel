{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Download and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and saved as 'shakespeare.txt'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "# Download Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the file\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Dataset downloaded and saved as 'shakespeare.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove unnecessary whitespace and newlines\n",
    "data = text.replace(\"\\n\", \" \")\n",
    "\n",
    "print(f\"Dataset length: {len(data)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Build n-gram Model (n=2 and n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bigrams: 1318\n",
      "Total unique trigrams: 10033\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from the given text.\"\"\"\n",
    "    ngrams = [tuple(text[i : i + n]) for i in range(len(text) - n)]\n",
    "    return Counter(ngrams)\n",
    "\n",
    "# Generate bigrams (n=2) and trigrams (n=3)\n",
    "bigrams = generate_ngrams(data, 2)\n",
    "trigrams = generate_ngrams(data, 3)\n",
    "\n",
    "print(f\"Total unique bigrams: {len(bigrams)}\")\n",
    "print(f\"Total unique trigrams: {len(trigrams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Find the Most Frequent n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigram: [(('e', ' '), 29077)]\n",
      "Most common trigram: [((' ', 't', 'h'), 16237)]\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_ngrams(ngram_counter, top_n=1):\n",
    "    \"\"\"Return the most common n-grams.\"\"\"\n",
    "    return ngram_counter.most_common(top_n)\n",
    "\n",
    "most_common_bigrams = most_frequent_ngrams(bigrams, 1)\n",
    "most_common_trigrams = most_frequent_ngrams(trigrams, 1)\n",
    "\n",
    "print(f\"Most common bigram: {most_common_bigrams}\")\n",
    "print(f\"Most common trigram: {most_common_trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Find the Most Likely Next Character for Each n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely next characters for bigram ('e', ' '): Counter()\n",
      "Most likely next characters for trigram (' ', 't', 'h'): Counter()\n"
     ]
    }
   ],
   "source": [
    "def compute_next_char_probabilities(text, n):\n",
    "    \"\"\"Compute the probability distribution of the next character given an n-gram prefix.\"\"\"\n",
    "    ngram_dict = defaultdict(Counter)\n",
    "    for i in range(len(text) - n):\n",
    "        prefix = tuple(text[i : i + n - 1])  # (xt-1, xt-2, ..., xt-n+1)\n",
    "        next_char = text[i + n - 1]  # xt\n",
    "        ngram_dict[prefix][next_char] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    for prefix, counter in ngram_dict.items():\n",
    "        total_count = sum(counter.values())\n",
    "        for char in counter:\n",
    "            counter[char] /= total_count\n",
    "    \n",
    "    return ngram_dict\n",
    "\n",
    "# Compute next character probabilities for bigrams and trigrams\n",
    "bigram_next_char_probs = compute_next_char_probabilities(data, 2)\n",
    "trigram_next_char_probs = compute_next_char_probabilities(data, 3)\n",
    "\n",
    "# Show example output\n",
    "example_bigram_prefix = most_common_bigrams[0][0]  # Most common bigram prefix\n",
    "example_trigram_prefix = most_common_trigrams[0][0]  # Most common trigram prefix\n",
    "\n",
    "print(f\"Most likely next characters for bigram {example_bigram_prefix}: {bigram_next_char_probs[example_bigram_prefix]}\")\n",
    "print(f\"Most likely next characters for trigram {example_trigram_prefix}: {trigram_next_char_probs[example_trigram_prefix]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Generate Text using the n-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text with Bigrams:\n",
      "hef GEShaneand thinove the aize se pe  mauts haloromay   ay PHE: waine thene wdiedlat we sti'd atouss I GLLar, nd d. hankind uge s. beakeand Whet Chisataifr's louchowesat hel. atatineaseen vas odeme: D\n",
      "haimasho Cing; cks JO: her; RYo ad. toncaitilo- t by ULey hithand h ofit Y ht mu be bo t: cepasto! notherbaneive: KIOLERoswhin had myortou bo--f, ad'swianghaveallos h th, me wn mert tr, ngoralitre dsh \n",
      "hechin BABe s bun o D boupuryssatous uablfldofay. hor, Mit? heind helist: sto, a mettiofove DUETRELUCUCE: O: y CHato Whoud g INGOUTHoul yort m Fofle Gab, la hasist's, su plld IUThaw clonothiouepfeck yo\n",
      "\n",
      "Generated Text with Trigrams:\n",
      "o-mot thesse ings frefaughbastrust Myse dand So behen the car frouch RIA: I'llorratellich floondstiong'd wrought by not the O Than's hour mur lach of claid bleachal, aren, What so makinexce, Andoot ing,\n",
      "o-mort mour of ereir be thy, wou re meneete Thind bat! Blose, MAR: How DUKE O, ithen hemakentles din hat, And a ping 't RICKING ESTERCIUS: Com re O nown, astrafecome wou and welf, Than madukes, thus.  Y\n",
      "o-may con.  And frow heenit, an him bot, I ch th cany to yours, st.  DUKENRY Pad mis this bre.  GREMIO: Io of a grephes dear.   But whord butheyettill roso, doose vand is we your cand no have to doger i\n"
     ]
    }
   ],
   "source": [
    "def generate_text(ngram_dict, seed, length=100):\n",
    "    \"\"\"Generate text using an n-gram probability distribution.\"\"\"\n",
    "    generated = list(seed)\n",
    "    for _ in range(length):\n",
    "        prefix = tuple(generated[-(len(seed)):])  # Match the prefix length\n",
    "        if prefix in ngram_dict:\n",
    "            next_char = random.choices(\n",
    "                list(ngram_dict[prefix].keys()), \n",
    "                weights=ngram_dict[prefix].values()\n",
    "            )[0]\n",
    "            generated.append(next_char)\n",
    "        else:\n",
    "            break  # Stop if no continuation found\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Generate three paragraphs of text using bigrams and trigrams\n",
    "bigram_seed = random.choice(list(bigram_next_char_probs.keys()))\n",
    "trigram_seed = random.choice(list(trigram_next_char_probs.keys()))\n",
    "\n",
    "print(\"\\nGenerated Text with Bigrams:\")\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "\n",
    "print(\"\\nGenerated Text with Trigrams:\")\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Warmup for neural network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Character Level Text Generation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Shakespeare dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character vocabulary\n",
    "chars = sorted(set(text))\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "# Convert text to indices\n",
    "encoded_text = [char2idx[char] for char in text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define GRU-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增加一個 `init_hidden()` 方法到 `GRUModel`\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        初始化隱藏層，大小應該是 (num_layers, batch_size, hidden_size)\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(encoded_text, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(encoded_text) - seq_length):\n",
    "        sequences.append(encoded_text[i:i+seq_length])\n",
    "        targets.append(encoded_text[i+1:i+seq_length+1])\n",
    "    return torch.tensor(sequences), torch.tensor(targets)\n",
    "\n",
    "seq_length = 100\n",
    "sequences, targets = create_sequences(encoded_text, seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0295\n",
      "Epoch 2/10, Loss: 0.0299\n",
      "Epoch 3/10, Loss: 0.0298\n",
      "Epoch 4/10, Loss: 0.0314\n",
      "Epoch 5/10, Loss: 0.0342\n",
      "Epoch 6/10, Loss: 0.0365\n",
      "Epoch 7/10, Loss: 0.0375\n",
      "Epoch 8/10, Loss: 0.0380\n",
      "Epoch 9/10, Loss: 0.0387\n",
      "Epoch 10/10, Loss: 0.0391\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, data, targets, num_epochs=10, batch_size=64, learning_rate=0.003):\n",
    "    \"\"\"\n",
    "    訓練 GRU 模型，並使用 CrossEntropyLoss 作為損失函數。\n",
    "\n",
    "    Args:\n",
    "    - model: 定義的 GRU 模型\n",
    "    - data: 訓練輸入數據 (Tensor)\n",
    "    - targets: 對應的標籤數據 (Tensor)\n",
    "    - num_epochs: 訓練的迭代次數 (default: 10)\n",
    "    - batch_size: 每個 batch 的大小 (default: 64)\n",
    "    - learning_rate: 學習率 (default: 0.003)\n",
    "    \"\"\"\n",
    "\n",
    "    # 設定設備\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)  # 把模型移到 GPU (如果有的話)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            inputs = data[i:i+batch_size].to(device)   # 確保在正確的設備上\n",
    "            labels = targets[i:i+batch_size].to(device)\n",
    "\n",
    "            # 確保 hidden 的 batch_size 是正確的\n",
    "            current_batch_size = inputs.shape[0]  # 最後一個 batch 可能小於 batch_size\n",
    "            hidden = model.init_hidden(current_batch_size).to(device)  \n",
    "\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "            \n",
    "            # 前向傳播\n",
    "            output, hidden = model(inputs, hidden)\n",
    "\n",
    "            # 計算 loss\n",
    "            loss = criterion(output.view(-1, len(chars)), labels.view(-1))\n",
    "            \n",
    "            # 反向傳播 & 更新參數\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# 初始化模型並訓練\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUModel(len(chars), embed_size=128, hidden_size=256, num_layers=2).to(device)\n",
    "train_model(model, sequences, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Generate and print example text\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTo be or not to be\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_str, length)\u001b[0m\n\u001b[1;32m     10\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m start_str\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(length):\n\u001b[0;32m---> 13\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# 取得最後一個時間步的輸出，並轉換為機率\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/DGM_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DGM_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mGRUModel.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, hidden):\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m---> 13\u001b[0m     out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out, hidden\n",
      "File \u001b[0;32m~/DGM_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DGM_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DGM_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py:1139\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1143\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, length=200):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 初始化 hidden，並確保它在正確的設備上\n",
    "    hidden = model.init_hidden(batch_size=1).to(device)\n",
    "\n",
    "    # 轉換 start_str 為索引，確保 input_seq 在正確設備上\n",
    "    input_seq = torch.tensor([char2idx[char] for char in start_str], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated_text = start_str\n",
    "\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # 取得最後一個時間步的輸出，並轉換為機率\n",
    "        probs = torch.nn.functional.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy()\n",
    "        next_char_idx = np.random.choice(len(chars), p=probs.flatten())\n",
    "\n",
    "        # 更新生成文本\n",
    "        generated_text += idx2char[next_char_idx]\n",
    "\n",
    "        # 更新 input_seq，確保它在 GPU 上\n",
    "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Generate and print example text\n",
    "print(generate_text(model, 'To be or not to be', length=200))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
