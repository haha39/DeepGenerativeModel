{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download and Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and saved as 'shakespeare.txt'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "# Download Shakespeare dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the file\n",
    "with open(\"shakespeare.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "print(\"Dataset downloaded and saved as 'shakespeare.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Remove unnecessary whitespace and newlines\n",
    "data = text.replace(\"\\n\", \" \")\n",
    "\n",
    "print(f\"Dataset length: {len(data)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build n-gram Model (n=2 and n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Show how many distinct tuples are there in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique bigrams: 1318\n",
      "Total unique trigrams: 10033\n"
     ]
    }
   ],
   "source": [
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from the given text.\"\"\"\n",
    "    ngrams = [tuple(text[i : i + n]) for i in range(len(text) - n)]\n",
    "    return Counter(ngrams)\n",
    "\n",
    "# Generate bigrams (n=2) and trigrams (n=3)\n",
    "bigrams = generate_ngrams(data, 2)\n",
    "trigrams = generate_ngrams(data, 3)\n",
    "\n",
    "print(f\"Total unique bigrams: {len(bigrams)}\")\n",
    "print(f\"Total unique trigrams: {len(trigrams)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Find the Most Frequent n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common bigram: [(('e', ' '), 29077)]\n",
      "Most common trigram: [((' ', 't', 'h'), 16237)]\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_ngrams(ngram_counter, top_n=1):\n",
    "    \"\"\"Return the most common n-grams.\"\"\"\n",
    "    return ngram_counter.most_common(top_n)\n",
    "\n",
    "most_common_bigrams = most_frequent_ngrams(bigrams, 1)\n",
    "most_common_trigrams = most_frequent_ngrams(trigrams, 1)\n",
    "\n",
    "print(f\"Most common bigram: {most_common_bigrams}\")\n",
    "print(f\"Most common trigram: {most_common_trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Find the Most Three Likely Next Character for Each n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 most likely next characters for bigram ('e',): [(' ', 0.3073321283994462), ('r', 0.1244147086491), ('n', 0.07999069875595861)]\n",
      "Top 3 most likely next characters for trigram (' ', 't'): [('h', 0.672813160402768), ('o', 0.20453321178469316), ('r', 0.03488998466829652)]\n"
     ]
    }
   ],
   "source": [
    "def compute_next_char_probabilities(text, n):\n",
    "    \"\"\"Compute the probability distribution of the next character given an n-gram prefix.\"\"\"\n",
    "    ngram_dict = defaultdict(Counter)\n",
    "    for i in range(len(text) - (n-1)):\n",
    "        prefix = tuple(text[i : i + n - 1])  # (xt-1, xt-2, ..., xt-n+1)\n",
    "        next_char = text[i + n - 1]  # xt\n",
    "        ngram_dict[prefix][next_char] += 1\n",
    "    \n",
    "    # Convert counts to probabilities\n",
    "    for prefix, counter in ngram_dict.items():\n",
    "        total_count = sum(counter.values())\n",
    "        for char in counter:\n",
    "            counter[char] /= total_count\n",
    "    \n",
    "    return ngram_dict\n",
    "\n",
    "# Compute next character probabilities for bigrams and trigrams\n",
    "bigram_next_char_probs = compute_next_char_probabilities(data, 2)\n",
    "trigram_next_char_probs = compute_next_char_probabilities(data, 3)\n",
    "\n",
    "# Show example output\n",
    "example_bigram_prefix = most_common_bigrams[0][0][:-1]   # Most common bigram prefix\n",
    "example_trigram_prefix = most_common_trigrams[0][0][:-1]   # Most common trigram prefix\n",
    "\n",
    "top_3_bigram_chars = bigram_next_char_probs[example_bigram_prefix].most_common(3)\n",
    "top_3_trigram_chars = trigram_next_char_probs[example_trigram_prefix].most_common(3)\n",
    "\n",
    "print(f\"Top 3 most likely next characters for bigram {example_bigram_prefix}: {top_3_bigram_chars}\")\n",
    "print(f\"Top 3 most likely next characters for trigram {example_trigram_prefix}: {top_3_trigram_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Generate Text using the n-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text with Bigrams:\n",
      "t dorday s: the cley tigano hive  I A: alun pite, hyou. s? for suremouour Isou; l Thoucu led h nstha w alouthos blla Are doulisod ms n. BUnd farg ay yong S: Iffay ire tisathavity conon pluthe skig thob\n",
      "the te Fow AUCETARO: fag wer, w, s g's: he shototorag d I for isoligms ane th TIng gee d ar, outotou curvee fany atsshitajelyopr, l. te by athafrty w nst bie the HOFRI ce: co lloordinired ke t pousatou\n",
      "thif y! Bof Th. tthy th s ind fo st ch y aist. Mat GLIsit whese arant Y a areff. t wan, ifur thown t Tofainpeak! hes, pp, thisucase, an; tal, timo llind stind fa hathele t testhege e d toman oches  The\n",
      "\n",
      "Generated Text with Trigrams:\n",
      "MPSABET: I a rearms, I to lions! wan th ented to You and hou love ind is, weet quare dot apprisladed yousantire so ase: And forbecompait fou anseelf himbe pre wilt ne: blord: Aband I seentills take heed\n",
      "MPSABET: Not firm That Wary min and to mor tow on, sile havest.  PER: All not? O, him deam an, in eavers, wity, so und annamearwithe math to nou, spow they yourns, O, What aught us maretten an have swas\n",
      "MPSABELLAUMNIUS: HOMPET: Witheacres trier res me, hostres upow fores heedid mend fromen of Withee, That man menceir, Nor dearwill so, Whou reall deas Will for duar your day old namennot he youly sir, wo\n"
     ]
    }
   ],
   "source": [
    "def generate_text(ngram_dict, seed, length=100):\n",
    "    \"\"\"Generate text using an n-gram probability distribution.\"\"\"\n",
    "    generated = list(seed)\n",
    "    for _ in range(length):\n",
    "        prefix = tuple(generated[-(len(seed)):])  # Match the prefix length\n",
    "        if prefix in ngram_dict:\n",
    "            next_char = random.choices(\n",
    "                list(ngram_dict[prefix].keys()), \n",
    "                weights=ngram_dict[prefix].values()\n",
    "            )[0]\n",
    "            generated.append(next_char)\n",
    "        else:\n",
    "            break  # Stop if no continuation found\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Generate three paragraphs of text using bigrams and trigrams\n",
    "bigram_seed = random.choice(list(bigram_next_char_probs.keys()))\n",
    "trigram_seed = random.choice(list(trigram_next_char_probs.keys()))\n",
    "\n",
    "print(\"\\nGenerated Text with Bigrams:\")\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "print(generate_text(bigram_next_char_probs, bigram_seed, length=200))\n",
    "\n",
    "print(\"\\nGenerated Text with Trigrams:\")\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))\n",
    "print(generate_text(trigram_next_char_probs, trigram_seed, length=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmup for neural network and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Character Level Text Generation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Shakespeare dataset\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character vocabulary\n",
    "chars = sorted(set(text))\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "# Convert text to indices\n",
    "encoded_text = [char2idx[char] for char in text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define GRU-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(encoded_text, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(encoded_text) - seq_length):\n",
    "        sequences.append(encoded_text[i:i+seq_length])\n",
    "        targets.append(encoded_text[i+1:i+seq_length+1])\n",
    "    return torch.tensor(sequences), torch.tensor(targets)\n",
    "\n",
    "seq_length = 100\n",
    "sequences, targets = create_sequences(encoded_text, seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0296\n",
      "Epoch 2/10, Loss: 0.0297\n",
      "Epoch 3/10, Loss: 0.0302\n",
      "Epoch 4/10, Loss: 0.0307\n",
      "Epoch 5/10, Loss: 0.0305\n",
      "Epoch 6/10, Loss: 0.0307\n",
      "Epoch 7/10, Loss: 0.0319\n",
      "Epoch 8/10, Loss: 0.0342\n",
      "Epoch 9/10, Loss: 0.0341\n",
      "Epoch 10/10, Loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, data, targets, num_epochs=10, batch_size=64, learning_rate=0.003):\n",
    "    \"\"\"\n",
    "    Train the GRU model and use CrossEntropyLoss as the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            inputs = data[i:i+batch_size].to(device)   # Make sure it's on the right device\n",
    "            labels = targets[i:i+batch_size].to(device)\n",
    "\n",
    "            # Make sure the hidden batch_size is correct.\n",
    "            current_batch_size = inputs.shape[0]\n",
    "            hidden = model.init_hidden(current_batch_size).to(device)  \n",
    "\n",
    "            optimizer.zero_grad()  # Empty the gradient\n",
    "            \n",
    "            # forward propagation\n",
    "            output, hidden = model(inputs, hidden)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(output.view(-1, len(chars)), labels.view(-1))\n",
    "            \n",
    "            # Reverse Spread & Update Parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(data):.4f}')\n",
    "\n",
    "\n",
    "\n",
    "# Initialising the model and training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GRUModel(len(chars), embed_size=128, hidden_size=256, num_layers=2).to(device)\n",
    "train_model(model, sequences, targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to bead a didmy and amp in Datoubth baspes? Wray. Whiss apre have and spit? bet knon Drsysich so Mays as ay? I at awere rocpocaltempite daughan she as id and mness king-not, I shasta;\n",
      "AThout they;\n",
      "Thy stak\n",
      "\n",
      "\n",
      "I at they plast makaning these of not youk,-Dard, such by lats maulible all nal?\n",
      "\n",
      "ANTONIO:\n",
      "Why dannt no bis me Dhere? -not pay? and you? spir? Saper ats and, arpe shall buph spit not didank all.\n",
      "\n",
      "SEBAS\n",
      "\n",
      "\n",
      "and leth enget ave? the daugcess eme these mink phereinconst soppfar a that atting appets?\n",
      "\n",
      "SEBASTIAN:\n",
      "Whe then onlencice,--a neve? add--- and nentiugle.\n",
      "\n",
      "SEBASTIAN:\n",
      "No take was's teedd.\n",
      "\n",
      "ANTONIO:\n",
      "That a\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_str, length=200):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialise the hidden and make sure it is on the correct device\n",
    "    hidden = model.init_hidden(batch_size=1).to(device)\n",
    "\n",
    "    # Convert start_str to index to make sure input_seq is on the right device\n",
    "    input_seq = torch.tensor([char2idx[char] for char in start_str], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    generated_text = start_str\n",
    "\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input_seq, hidden)\n",
    "\n",
    "        # Get the output of the last time step and convert it to a chance\n",
    "        probs = torch.nn.functional.softmax(output[:, -1, :], dim=-1).detach().cpu().numpy()\n",
    "        next_char_idx = np.random.choice(len(chars), p=probs.flatten())\n",
    "\n",
    "        # Update Generated Text\n",
    "        generated_text += idx2char[next_char_idx]\n",
    "\n",
    "        # Update input_seq to make sure it's on the GPU\n",
    "        input_seq = torch.tensor([[next_char_idx]], dtype=torch.long).to(device)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Generate and print example text\n",
    "print(generate_text(model, 'To be or not to be', length=200))\n",
    "print(\"\\n\")\n",
    "print(generate_text(model, 'I', length=200))\n",
    "print(\"\\n\")\n",
    "print(generate_text(model, 'and', length=200))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
